Classification with SPECT dataset

SPECT is a good data set for testing ML algorithms; it has 267 instances that are descibed by 23 binary attributes.

The dataset describes diagnosing of cardiac Single Proton Emission Computed Tomography (SPECT) images.

Each of the patients is classified into two categories: normal and abnormal.

The database of 267 SPECT image sets (patients) was processed to extract features that summarize the original SPECT images.

As a result, 44 continuous feature pattern was created for each patient.

The pattern was further processed to obtain 22 binary feature patterns.

The CLIP3 algorithm was used to generate classification rules from these patterns.

The CLIP3 algorithm generated rules that were 84.0% accurate (as compared with cardilogists' diagnoses).

. Number of Instances: 267

. Number of Attributes: 23 (22 binary + 1 binary class)

Class Distribution:

-- entire data Class # examples 0 55 1 212

-- training dataset Class # examples 0 40 1 40

-- testing dataset Class # examples 0 15 1 172

Attribute Information:

OVERALL_DIAGNOSIS: 0,1 (class attribute, binary)
F1: 0,1 (the partial diagnosis 1, binary)
F2: 0,1 (the partial diagnosis 2, binary)
F3: 0,1 (the partial diagnosis 3, binary)
F4: 0,1 (the partial diagnosis 4, binary)
F5: 0,1 (the partial diagnosis 5, binary)
F6: 0,1 (the partial diagnosis 6, binary)
F7: 0,1 (the partial diagnosis 7, binary)
F8: 0,1 (the partial diagnosis 8, binary)
F9: 0,1 (the partial diagnosis 9, binary)
F10: 0,1 (the partial diagnosis 10, binary)
F11: 0,1 (the partial diagnosis 11, binary)
F12: 0,1 (the partial diagnosis 12, binary)
F13: 0,1 (the partial diagnosis 13, binary)
F14: 0,1 (the partial diagnosis 14, binary)
F15: 0,1 (the partial diagnosis 15, binary)
F16: 0,1 (the partial diagnosis 16, binary)
F17: 0,1 (the partial diagnosis 17, binary)
F18: 0,1 (the partial diagnosis 18, binary)
F19: 0,1 (the partial diagnosis 19, binary)
F20: 0,1 (the partial diagnosis 20, binary)
F21: 0,1 (the partial diagnosis 21, binary)
F22: 0,1 (the partial diagnosis 22, binary)

### IMPORT LIBRARIES AND SET UP DATA ###

import os
import csv
import pandas as pd
import numpy as np
from numpy import recfromcsv
import urllib

url="https://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train"
raw_data_train=urllib.request.urlopen(url)
train_data=np.loadtxt(raw_data_train,delimiter=',')
train_data

X = train_data[: , 1:]                                 # Train data features
y = train_data[: , 0]                                  # Train data target outcomes (desired labels)
print(X.shape)
print(y.shape)


url="https://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.test"
raw_data_test=urllib.request.urlopen(url)
test_data=np.loadtxt(raw_data_test,delimiter=',')
test_data


X_test = test_data[: , 1:]                            # Test data features
y_test = test_data[: , 0]                             # Test data target outcomes (desired labels)

print(X_test.shape)
print(y_test.shape)


# Implement a perceptron network #

accuracy_vector=np.arange(0, 1, dtype=np.float)              # creates a float data type vector of 1 element, starting at 0   
print(accuracy_vector)

for num in range(100):
  
        random_num = randint(1, 100)                                    # Pick a random number between 1 and 100.

        print(random_num)                                     

        class Perceptron(object):

            def __init__(self, input_size, lr=0.1, epochs=1000):
                self.W = np.zeros(input_size+1)                         # define the weights vector; add one for the BIAS
                self.epochs = epochs
                self.lr = lr

            def activation_fn(self, x):                   # we are defining a function that will be applied to z; x here only 
                return 1 if x >0 else 0                       # indicates the argument of the function. #

            def predict(self, x):
                z = self.W.T.dot(x)                  # z is dot product between the initial weight vector, made all of zeroes, 
                                                     # transposed, and the "small x" vector.
                                                        # self.W gets updated at each iteration, and so does z. #
                a = self.activation_fn(z)                     # the activation function defined above. #
                return a

            def fit(self, X, d):
                for _ in range(self.epochs):                # the underscore is a 'throwaway variable' #
                    for i in range(d.shape[0]):         # d.shape = [r, c], so d.shape[0] returns the rows value of the shape 
                                                                  # vector of d.
                                                              # in this case, d is 1-dimensional and therefore d.shape = [#,] 
                                                                  # where # is the
                                                              # number of elements in array d. Therefore, (d.shape[0]) returns 
                                                                  # that number. because d has 80 elements, this does
                                                                  #  "for i that goes from 0 to 79". #
                        x = np.insert(X[i], 0, random_num)    # inserts in element i of array X, at position 0, the value 4:
                                                          # this inserts the THRESHOLD(=random_num) into the input when   
                                                          # performing the weight update; NB: x = np.inserts(X[i], position,  
                                                              # value) gives back a VECTOR x; this process is iterated 
                                                              # through all i examples. We may change the third argument 
                                                              # of the insertfunction to any random number. #
                        y = self.predict(x)                   # returns either 0 or 1, based on whether the activation function 
                                                                  # a = 0 or a = 1. #
                        e = d[i] - y                          # d is desired, y is predicted, so e is the error #
                        self.W = self.W + self.lr * e * x     # weight update rule #
                        self.lr = self.lr - (self.lr*0.1/self.epochs)  # if Learning Rate decreases with epochs it will converge!

        if __name__ == '__main__':
            X = train_data[: , 1:]
            d = train_data[: , 0]

            perceptron = Perceptron(input_size=22)
            perceptron.fit(X, d)
            print(perceptron.W)


            # TEST #

        X = X_test
        d = y_test                                           # d stands for desired (test)
        w = perceptron.W
        outcome = np.zeros(len(d))                            # create an empty vector that will be filled by our outcome predictions 



        N = (X.shape[0])
        X = np.c_[ np.ones(N), X]                              # we need to insert a column with all 1's to the left of X
                                                                   # just like above we added a column to the X train matrix.
        print(X.shape)
        print(w.shape)


        for i in range(X.shape[0]):               
            outcome[i] = w.T.dot(X[i])


        outcome_bin = outcome



        outcome_bin[outcome_bin > 0] = 1
        outcome_bin[outcome_bin <= 0] = 0

        print(outcome_bin == y_test)

        print(outcome_bin)
        print(y_test)

        print(d.shape)    
        print(outcome.shape)

        accuracy=sum(outcome_bin == y_test)/(len(y_test))
        
                
        accuracy_vector = np.insert(accuracy_vector, num, accuracy)
        
        print(num)
        print(accuracy_vector)
        print(accuracy_vector[num])
        print(accuracy_vector[num-1])
        
        if accuracy_vector[num]<accuracy_vector[num-1]:
            print("the accuracy of the algorithm is", accuracy_vector[num-1], "using a threshold of", random_num)    
            break
        
